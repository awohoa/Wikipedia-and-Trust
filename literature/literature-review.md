# Research Articles 


1. **Implications of generative AI for knowledge integrity on Wikipedia**
Ford, Heather; Davis Michael (2024). ["Implications of generative AI for knowledge integrity on Wikipedia"](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Implications+of+generative+AI+for+knowledge+integrity+on+Wikipedia&btnG=) | [pdf](https://wikiworkshop.org/2025/paper/wikiworkshop_2025_paper_54.pdf)

This article directly connects to our research topic because it examines how AI challenges Wikipedia's knowledge integrity. The methods used in this research include analyzing discussion forums, blogs, and community debates, and conducting interviews with Wikimedians between July 2022 and July 2023. Some benefits found were that many editors found AI tools helpful in facilitating tasks such as assisting with drafting articles, summarizing sources, and translating. However, internal risks arise, such as the proliferation of low-quality/AI-generated content that is difficult to fact-check. Maintaining verifiability and connection between claims and sources becomes increasingly difficult when content is AI-generated. 


2. **Will large language models diminish the reliance on Wikipedia?**
Wagner, Christian; Jiang, Lin (2025) ["Death by AI: Will large language models diminish Wikipedia?"](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Death+by+AI%3A+Will+large+language+models+diminish+Wikipedia%3F&btnG=) | [pdf](https://asistdl.onlinelibrary.wiley.com/doi/epdf/10.1002/asi.24975) 

This article supports our research by directly examining how AI weakens Wikipedia's relevance. The article's main argument is that generative AI and LLMs threaten Wikipedia's value because they will reduce human contributors from editing, as they will have less incentive to contribute. A feedback model is used to show how AI disrupts the relationship between readers and contributors, as AI systems are overpowering human participation. Since W directly depends on contributors contributing without any direct reward (known as "selfish altruism"), this is weakened with AI. This directly challenges the future of W, as AI-generated content can overpower, and content will degrade as fewer contributors check/update information.

5. **Artificial Intelligence, Trust, and Relevance of Online Knowledge**
   
Vetter, Matthew A.; Jiang, Jialei; McDowell, Zachary J. (2025).
["An endangered species: how LLMs threaten Wikipedia’s sustainability."](https://link.springer.com/article/10.1007/s00146-025-02199-9)
[Google Scholar](https://scholar.google.com/scholar_lookup?title=An+endangered+species:+how+LLMs+threaten+Wikipedia%E2%80%99s+sustainability&author=Vetter&author=Jiang&author=McDowell&publication_year=2025) | [pdf](https://link.springer.com/content/pdf/10.1007/s00146-025-02199-9.pdf)  

This article connects to our main question because it addresses both trust and relevance. The authors point out that AI systems often pull from Wikipedia without giving much credit, which raises the question about whether people can really trust how that information is being used. The article also talks about sustainability. If people stop going to Wikipedia and just use AI instead, fewer people will contribute and the platform could lose what makes it special as a community-driven project. What’s especially helpful about this article is that it doesn’t just focus on problems happening right now, it also warns about what could happen in the future and suggests ways to keep Wikipedia both trustworthy and relevant.

4. **Perceived Trustworthiness of Wikipedia Pages**

Areia, Carlos; Burton, Kath; Taylor, Mike; Watkinson, Charles. (2025).
["Research citations building trust in Wikipedia: Results from a survey of published authors."](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0320334)
[Google Scholar](https://scholar.google.com/scholar_lookup?title=Research+citations+building+trust+in+Wikipedia:+Results+from+a+survey+of+published+authors&author=Areia&author=Burton&author=Taylor&author=Watkinson&publication_year=2025) | [pdf](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0320334&type=printable)  

This article examines how researchers perceive the trustworthiness of Wikipedia citations and how the platform represents their work. Using a large survey of over 700 published authors whose work had been cited on Wikipedia, the study explored sentiment towards Wikipedia as a source of reliable information. The findings show that authors generally view research citations on Wikipedia positively, especially when proper engagement practices are followed. It also highlights differences in trust depending on publication type (articles vs. books) and academic discipline (Humanities and Social Sciences vs. STEM). This is relevant to our project because it demonstrates that trustworthiness on Wikipedia is influenced by specific page properties, such as the quality and representation of citations, and suggests that user perception plays a key role in maintaining Wikipedia’s credibility.
